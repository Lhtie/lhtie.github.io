{
    "sourceFile": "_bibliography/papers.bib",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1732164224777,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1732164246461,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,15 +29,15 @@\n url={https://openreview.net/forum?id=9HdQr68Zyl}\n }\n \n @misc{zhu2024multilingualmachinetranslationlarge,\n-      title={Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis}, \n-      author={Wenhao Zhu and Hongyi Liu and Qingxiu Dong and Jingjing Xu and Shujian Huang and Lingpeng Kong and Jiajun Chen and Lei Li},\n-      year={2024},\n-      eprint={2304.04675},\n-      archivePrefix={arXiv},\n-      primaryClass={cs.CL},\n-      url={https://arxiv.org/abs/2304.04675}, \n+    title={Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis}, \n+    author={Wenhao Zhu and Hongyi Liu and Qingxiu Dong and Jingjing Xu and Shujian Huang and Lingpeng Kong and Jiajun Chen and Lei Li},\n+    year={2024},\n+    eprint={2304.04675},\n+    archivePrefix={arXiv},\n+    primaryClass={cs.CL},\n+    url={https://arxiv.org/abs/2304.04675}, \n }\n \n ---\n \n"
                },
                {
                    "date": 1732164286646,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,6 +38,16 @@\n     primaryClass={cs.CL},\n     url={https://arxiv.org/abs/2304.04675}, \n }\n \n+@misc{xu2022gotuningimprovingzeroshotlearning,\n+      title={Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language Models}, \n+      author={Jingjing Xu and Qingxiu Dong and Hongyi Liu and Lei Li},\n+      year={2022},\n+      eprint={2212.10461},\n+      archivePrefix={arXiv},\n+      primaryClass={cs.CL},\n+      url={https://arxiv.org/abs/2212.10461}, \n+}\n+\n ---\n \n"
                },
                {
                    "date": 1732164297694,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,25 +29,25 @@\n url={https://openreview.net/forum?id=9HdQr68Zyl}\n }\n \n @misc{zhu2024multilingualmachinetranslationlarge,\n-        title={Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis}, \n-        author={Wenhao Zhu and Hongyi Liu and Qingxiu Dong and Jingjing Xu and Shujian Huang and Lingpeng Kong and Jiajun Chen and Lei Li},\n-        year={2024},\n-        eprint={2304.04675},\n-        archivePrefix={arXiv},\n-        primaryClass={cs.CL},\n-        url={https://arxiv.org/abs/2304.04675}, \n+    title={Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis}, \n+    author={Wenhao Zhu and Hongyi Liu and Qingxiu Dong and Jingjing Xu and Shujian Huang and Lingpeng Kong and Jiajun Chen and Lei Li},\n+    year={2024},\n+    eprint={2304.04675},\n+    archivePrefix={arXiv},\n+    primaryClass={cs.CL},\n+    url={https://arxiv.org/abs/2304.04675}, \n }\n \n @misc{xu2022gotuningimprovingzeroshotlearning,\n-      title={Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language Models}, \n-      author={Jingjing Xu and Qingxiu Dong and Hongyi Liu and Lei Li},\n-      year={2022},\n-      eprint={2212.10461},\n-      archivePrefix={arXiv},\n-      primaryClass={cs.CL},\n-      url={https://arxiv.org/abs/2212.10461}, \n+    title={Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language Models}, \n+    author={Jingjing Xu and Qingxiu Dong and Hongyi Liu and Lei Li},\n+    year={2022},\n+    eprint={2212.10461},\n+    archivePrefix={arXiv},\n+    primaryClass={cs.CL},\n+    url={https://arxiv.org/abs/2212.10461}, \n }\n \n ---\n \n"
                },
                {
                    "date": 1732164433854,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,54 @@\n+---\n+\n+@inproceedings{liu-etal-2024-named,\n+    title = \"Named Entity Recognition Under Domain Shift via Metric Learning for Life Sciences\",\n+    author = \"Liu, Hongyi  and\n+      Wang, Qingyun  and\n+      Karisani, Payam  and\n+      Ji, Heng\",\n+    editor = \"Duh, Kevin  and\n+      Gomez, Helena  and\n+      Bethard, Steven\",\n+    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n+    month = jun,\n+    year = \"2024\",\n+    address = \"Mexico City, Mexico\",\n+    publisher = \"Association for Computational Linguistics\",\n+    url = \"https://aclanthology.org/2024.naacl-long.1\",\n+    doi = \"10.18653/v1/2024.naacl-long.1\",\n+    pages = \"1--21\",\n+    abstract = \"Named entity recognition is a key component of Information Extraction (IE), particularly in scientific domains such as biomedicine and chemistry, where large language models (LLMs), e.g., ChatGPT, fall short. We investigate the applicability of transfer learning for enhancing a named entity recognition model trained in the biomedical domain (the source domain) to be used in the chemical domain (the target domain). A common practice for training such a model in a few-shot learning setting is to pretrain the model on the labeled source data, and then, to finetune it on a hand-full of labeled target examples. In our experiments, we observed that such a model is prone to mislabeling the source entities, which can often appear in the text, as the target entities. To alleviate this problem, we propose a model to transfer the knowledge from the source domain to the target domain, but, at the same time, to project the source entities and target entities into separate regions of the feature space. This diminishes the risk of mislabeling the source entities as the target entities. Our model consists of two stages: 1) entity grouping in the source domain, which incorporates knowledge from annotated events to establish relations between entities, and 2) entity discrimination in the target domain, which relies on pseudo labeling and contrastive learning to enhance discrimination between the entities in the two domains. We conduct our extensive experiments across three source and three target datasets, demonstrating that our method outperforms the baselines by up to 5{\\%} absolute value. Code, data, and resources are publicly available for research purposes: https://github.com/Lhtie/Bio-Domain-Transfer .\",\n+}\n+\n+@inproceedings{\n+lu2024opendomain,\n+    title={Open-Domain Text Evaluation via Contrastive Distribution Methods},\n+    author={Sidi Lu and Hongyi Liu and Asli Celikyilmaz and Tianlu Wang and Nanyun Peng},\n+    booktitle={Forty-first International Conference on Machine Learning},\n+    year={2024},\n+    abstract = \"Recent advancements in open-domain text generation, driven by the power of large pre-trained language models (LLMs), have demonstrated remarkable performance. However, assessing these models' generation quality remains a challenge. In this paper, we introduce a novel method for evaluating open-domain text generation called Contrastive Distribution Methods (CDM). Leveraging the connection between increasing model parameters and enhanced LLM performance, CDM creates a mapping from the contrast of two probabilistic distributions – one known to be superior to the other – to quality measures. We investigate CDM for open-domain text generation evaluation under two paradigms: 1) Generative CDM, which harnesses the contrast of two language models’ distributions to generate synthetic examples for training discriminator-based metrics; 2) Discriminative CDM, which directly uses distribution disparities between two language models for evaluation. Our experiments on coherence evaluation for multi-turn dialogue and commonsense evaluation for controllable generation demonstrate CDM’s superior correlate with human judgment than existing automatic evaluation metrics, highlighting the strong performance and generalizability of our approach. \"\n+url={https://openreview.net/forum?id=9HdQr68Zyl}\n+}\n+\n+@misc{zhu2024multilingualmachinetranslationlarge,\n+    title={Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis}, \n+    author={Wenhao Zhu and Hongyi Liu and Qingxiu Dong and Jingjing Xu and Shujian Huang and Lingpeng Kong and Jiajun Chen and Lei Li},\n+    year={2024},\n+    eprint={2304.04675},\n+    archivePrefix={arXiv},\n+    primaryClass={cs.CL},\n+    url={https://arxiv.org/abs/2304.04675}, \n+}\n+\n+@misc{xu2022gotuningimprovingzeroshotlearning,\n+    title={Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language Models}, \n+    author={Jingjing Xu and Qingxiu Dong and Hongyi Liu and Lei Li},\n+    year={2022},\n+    eprint={2212.10461},\n+    archivePrefix={arXiv},\n+    primaryClass={cs.CL},\n+    url={https://arxiv.org/abs/2212.10461}, \n+}\n+\n+---\n+\n"
                }
            ],
            "date": 1732164224777,
            "name": "Commit-0",
            "content": "---\n\n@inproceedings{liu-etal-2024-named,\n    title = \"Named Entity Recognition Under Domain Shift via Metric Learning for Life Sciences\",\n    author = \"Liu, Hongyi  and\n      Wang, Qingyun  and\n      Karisani, Payam  and\n      Ji, Heng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.1\",\n    doi = \"10.18653/v1/2024.naacl-long.1\",\n    pages = \"1--21\",\n    abstract = \"Named entity recognition is a key component of Information Extraction (IE), particularly in scientific domains such as biomedicine and chemistry, where large language models (LLMs), e.g., ChatGPT, fall short. We investigate the applicability of transfer learning for enhancing a named entity recognition model trained in the biomedical domain (the source domain) to be used in the chemical domain (the target domain). A common practice for training such a model in a few-shot learning setting is to pretrain the model on the labeled source data, and then, to finetune it on a hand-full of labeled target examples. In our experiments, we observed that such a model is prone to mislabeling the source entities, which can often appear in the text, as the target entities. To alleviate this problem, we propose a model to transfer the knowledge from the source domain to the target domain, but, at the same time, to project the source entities and target entities into separate regions of the feature space. This diminishes the risk of mislabeling the source entities as the target entities. Our model consists of two stages: 1) entity grouping in the source domain, which incorporates knowledge from annotated events to establish relations between entities, and 2) entity discrimination in the target domain, which relies on pseudo labeling and contrastive learning to enhance discrimination between the entities in the two domains. We conduct our extensive experiments across three source and three target datasets, demonstrating that our method outperforms the baselines by up to 5{\\%} absolute value. Code, data, and resources are publicly available for research purposes: https://github.com/Lhtie/Bio-Domain-Transfer .\",\n}\n\n@inproceedings{\nlu2024opendomain,\n    title={Open-Domain Text Evaluation via Contrastive Distribution Methods},\n    author={Sidi Lu and Hongyi Liu and Asli Celikyilmaz and Tianlu Wang and Nanyun Peng},\n    booktitle={Forty-first International Conference on Machine Learning},\n    year={2024},\nurl={https://openreview.net/forum?id=9HdQr68Zyl}\n}\n\n@misc{zhu2024multilingualmachinetranslationlarge,\n      title={Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis}, \n      author={Wenhao Zhu and Hongyi Liu and Qingxiu Dong and Jingjing Xu and Shujian Huang and Lingpeng Kong and Jiajun Chen and Lei Li},\n      year={2024},\n      eprint={2304.04675},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2304.04675}, \n}\n\n---\n\n"
        }
    ]
}